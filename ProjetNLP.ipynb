{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjetNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antoine723/ProjetNLP/blob/main/ProjetNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZNE_eBooDgL"
      },
      "source": [
        "- [Import bibliothèques et fichiers utiles](#0)\r\n",
        "- [Récupération commentaires API Yelp](#1)\r\n",
        "- [Traitement](#2)\r\n",
        "- [Création vocabulaire grâce aux données](#3)\r\n",
        "- [Transformation des mots en indices](#4)\r\n",
        "- [Création modèles](#5)\r\n",
        "- [Pipeline](#6)\r\n",
        "- [Test des modèles sur notre jeu de données](#7)\r\n",
        "- [Détermination du seuil selon les données observées](#8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz0GoIXymh0r"
      },
      "source": [
        "<a name=\"0\"></a>\r\n",
        "# Import bibliothèques et fichiers utiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLK8-Mgvzzrg",
        "outputId": "b8f6822d-cdba-436d-9b93-ae9c653a3133"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN10vLT_0P2y",
        "outputId": "e06c3800-0c07-4e92-c035-ba594da44752"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from gdrive.MyDrive.ProjetNLP.rnn_utils import *\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSV85DFHuEVj",
        "outputId": "379d7540-52fb-4151-c968-3e88b7516d97"
      },
      "source": [
        "pip install --pre gql[all]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gql[all]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/28/36e625703311aa14ac2c3e6679da3c826ec06f4fa7302c1baf093486c9e3/gql-3.0.0a5.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25hCollecting graphql-core<3.2,>=3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/4b/b38f0d94bfc47e3d1ba19dd8368d5808962891cfa1f7505072da4ff16acf/graphql_core-3.1.3-py3-none-any.whl (186kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 7.5MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 43.9MB/s \n",
            "\u001b[?25hCollecting aiohttp<3.8.0,>=3.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a6/d36302eba284f4f427dc288f6b3ecd7f89d739cfca206b80311d3158f6d9/aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.23 in /usr/local/lib/python3.7/dist-packages (from gql[all]) (2.23.0)\n",
            "Collecting websockets<9,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[?25hCollecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.6->gql[all]) (3.7.4.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.6->gql[all]) (2.10)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.7.1->gql[all]) (20.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.7.1->gql[all]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.23->gql[all]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.23->gql[all]) (2020.12.5)\n",
            "Building wheels for collected packages: gql\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-3.0.0a5-py2.py3-none-any.whl size=32709 sha256=d04483bbb684e7c625da9dedf50490a7c8b7f5f7a9a399203b1f5104170e32be\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/f4/5b/d4678b0d7c58b5b71cb44b849833ab4e6a0a9e4b6aad41b016\n",
            "Successfully built gql\n",
            "Installing collected packages: graphql-core, multidict, yarl, async-timeout, aiohttp, websockets, gql\n",
            "Successfully installed aiohttp-3.7.4 async-timeout-3.0.1 gql-3.0.0a5 graphql-core-3.1.3 multidict-5.1.0 websockets-8.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyr7Wo3-w3F2"
      },
      "source": [
        "import requests\r\n",
        "import json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObuheqgcmuOs"
      },
      "source": [
        "<a name=\"1\"></a>\r\n",
        "# Récupération commentaires API Yelp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caekNUL8uI2-",
        "outputId": "36aa4017-7c8c-485f-da42-0e9b71e26e8b"
      },
      "source": [
        "api_key='MI6JGndpuW3Kyc9cIjj5of9AhZUKET4JlGdRsiBnGZjL2ofbChjG0ksWdjXrHvdRN3cHosvu6bkDcvdOUCc5A_m-IAciq5kpjSqAxt5euSYLzB0q0vvWqmuyrI86YHYx'\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "\r\n",
        "dataset=json.loads(req.text)[\"reviews\"]\r\n",
        "x_test=[]\r\n",
        "for i in range(len(dataset)):\r\n",
        "  x_test.append(dataset[i][\"text\"])\r\n",
        "x_test2=x_test.copy()\r\n",
        "print(x_test)\r\n",
        "print(x_test2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\", 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...', 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...']\n",
            "[\"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\", 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...', 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-hOwDR3sIq"
      },
      "source": [
        "<a name=\"2\"></a>\r\n",
        "# Traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF6A8FMM47E9"
      },
      "source": [
        "def processing(data):\r\n",
        "  if (isinstance(data,str)):\r\n",
        "    #MINUSCULES\r\n",
        "    data=data.lower()\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "    data=tokenizer.tokenize(data)\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    data=[w for w in data if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    data=[stemmer.stem(w) for w in data]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    data=[Word_Lemmatizer.lemmatize(w) for w in data]\r\n",
        "\r\n",
        "  elif (isinstance(data,list)):\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=data[i].lower()\r\n",
        "\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=tokenizer.tokenize(data[i])\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[w for w in data[i] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "        \r\n",
        "\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[stemmer.stem(w) for w in data[i]]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[Word_Lemmatizer.lemmatize(w) for w in data[i]]\r\n",
        "    # print(data)\r\n",
        "\r\n",
        "  else :\r\n",
        "    #MINUSCULES\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=data[i][j].lower()\r\n",
        "\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=tokenizer.tokenize(data[i][j])\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[w for w in data[i][j] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "        \r\n",
        "\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[stemmer.stem(w) for w in data[i][j]]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[Word_Lemmatizer.lemmatize(w) for w in data[i][j]]\r\n",
        "    # print(data)\r\n",
        "\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjBDJZMm1_G"
      },
      "source": [
        "<a name=\"3\"></a>\r\n",
        "# Création du vocabulaire grâce aux données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykywsQVzQfCE"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "def defineVocab(data):\r\n",
        "  v=set()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      for h in range(len(data[i][j])):\r\n",
        "        v.add(data[i][j][h])\r\n",
        "\r\n",
        "  vocab = defaultdict(lambda: 0)\r\n",
        "  vocab['<PAD>'] = 1\r\n",
        "\r\n",
        "  j=2\r\n",
        "  for i in v:\r\n",
        "    vocab[i]=j\r\n",
        "    j+=1\r\n",
        "  return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTWcxxdtnBKU"
      },
      "source": [
        "<a name=\"4\"></a>\r\n",
        "# Transformation des mots en indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9z0zbQbO31u"
      },
      "source": [
        "def transformWords(data,vocab):\r\n",
        "  matrixData=[]\r\n",
        "  maxLen=0\r\n",
        "  if (isinstance(data,list)):\r\n",
        "    if (len(data)>1):\r\n",
        "      for i in range(len(data)):\r\n",
        "        tmp=[]\r\n",
        "        for j in data[i]:\r\n",
        "          if (j not in vocab.keys()):\r\n",
        "            tmp.append(0)\r\n",
        "          else:\r\n",
        "            tmp.append(vocab[j])\r\n",
        "        matrixData.append(tmp)\r\n",
        "    else:\r\n",
        "      for i in data:\r\n",
        "        if (i not in vocab.keys()):\r\n",
        "          matrixData.append(0)\r\n",
        "        else:\r\n",
        "          matrixData.append(vocab[i])\r\n",
        "  else:\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        if (maxLen < len(data[i][j])):\r\n",
        "          maxLen= len(data[i][j])\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        tmp=[]\r\n",
        "        for k in range(maxLen):\r\n",
        "          if (k >= len(data[i][j])):\r\n",
        "            tmp.append(vocab['<PAD>'])\r\n",
        "          else:\r\n",
        "            tmp.append(vocab[data[i][j][k]])\r\n",
        "        matrixData.append(tmp)\r\n",
        "  return matrixData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpSs72yjnM8u"
      },
      "source": [
        "<a name=\"5\"></a>\r\n",
        "# Création modèles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6eAwEM8HIB"
      },
      "source": [
        "def buildModelLSTM(vocab):\r\n",
        "  model=tf.keras.Sequential()\r\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=1))\r\n",
        "  model.add(tf.keras.layers.LSTM(128,activation=\"relu\"))\r\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hSGoAWX1eK"
      },
      "source": [
        "def buildModelSimpleRNN(vocab):\r\n",
        "  model=tf.keras.Sequential()\r\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=1))\r\n",
        "  model.add(tf.keras.layers.SimpleRNN(128,activation=\"relu\"))\r\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZJHW75PqsU2"
      },
      "source": [
        "def fitModel(data,model,negSize,posSize): #negSize et posSize = taille des données prises en compte pour le fit, respectivement des commentaires négatifs et positifs\r\n",
        "# Ex : si on crée le modèle en lui donnant 20 commentaires négatifs et 20 positifs, negSize=20 et posSize=20\r\n",
        "  y_train=np.concatenate([np.zeros((negSize,1)),np.ones((posSize,1))])\r\n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"adam\")\r\n",
        "  model.fit(data,y_train,epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEerdsq7nru9"
      },
      "source": [
        "<a name=\"6\"></a>\r\n",
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKB4t_HsvYWt"
      },
      "source": [
        "import os\r\n",
        "def final_LSTM(sentence):\r\n",
        "  threshold=0.2\r\n",
        "  model_file=\"LSTM_Model\"\r\n",
        "  if (not os.path.isdir(\"gdrive/MyDrive/ProjetNLP/\"+model_file)):\r\n",
        "    df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None)\r\n",
        "    x_train=np.array([df.iloc[0:1000][5],df.iloc[800000:801000][5]])\r\n",
        "    x_train=processing(x_train)\r\n",
        "    vocab=defineVocab(x_train)\r\n",
        "    df=pd.DataFrame(vocab.items(),columns=[\"Word\",\"Id\"])\r\n",
        "    df.to_csv(\"gdrive/MyDrive/ProjetNLP/vocab\")\r\n",
        "    x_train=transformWords(x_train,vocab)\r\n",
        "    x_train=np.array(x_train)\r\n",
        "    model=buildModelLSTM(vocab)\r\n",
        "    fitModel(x_train,model,1000,1000)\r\n",
        "    model.save(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "  else:\r\n",
        "    model=tf.keras.models.load_model(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "    vocab=pd.read_csv(\"gdrive/MyDrive/ProjetNLP/vocab\").set_index(\"Word\").to_dict()[\"Id\"]\r\n",
        "  # return model\r\n",
        "  sentence=processing(sentence)\r\n",
        "  sentence=transformWords(sentence,vocab)\r\n",
        "  sentence=np.array(sentence)\r\n",
        "  if (len(sentence.shape)==1 and len(sentence)==1):\r\n",
        "    if (model.predict(np.expand_dims(sentence,axis=0))<threshold):\r\n",
        "      commentTypes=\"Negative\"\r\n",
        "    else:\r\n",
        "      commentTypes=\"Positive\"\r\n",
        "  else:\r\n",
        "    commentTypes=[]\r\n",
        "    for i in range(len(sentence)):\r\n",
        "      if (model.predict(np.expand_dims(sentence[i],axis=0))<threshold):\r\n",
        "        commentTypes.append(\"Negative\")\r\n",
        "      else:\r\n",
        "        commentTypes.append(\"Positive\")\r\n",
        "  return commentTypes\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EErDV8ZNaM2H"
      },
      "source": [
        "import os\r\n",
        "def final_SimpleRNN(sentence):\r\n",
        "  threshold=0.4\r\n",
        "  model_file=\"SimpleRNN_Model\"\r\n",
        "  if (not os.path.isdir(\"gdrive/MyDrive/ProjetNLP/\"+model_file)):\r\n",
        "    df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None)\r\n",
        "    x_train=np.array([df.iloc[0:1000][5],df.iloc[800000:801000][5]])\r\n",
        "    x_train=processing(x_train)\r\n",
        "    vocab=defineVocab(x_train)\r\n",
        "    df=pd.DataFrame(vocab.items(),columns=[\"Word\",\"Id\"])\r\n",
        "    df.to_csv(\"gdrive/MyDrive/ProjetNLP/vocab\")\r\n",
        "    x_train=transformWords(x_train,vocab)\r\n",
        "    x_train=np.array(x_train)\r\n",
        "    model=buildModelSimpleRNN(vocab)\r\n",
        "    fitModel(x_train,model,1000,1000)\r\n",
        "    model.save(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "  else:\r\n",
        "    model=tf.keras.models.load_model(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "    vocab=pd.read_csv(\"gdrive/MyDrive/ProjetNLP/vocab\").set_index(\"Word\").to_dict()[\"Id\"]\r\n",
        "  # return model\r\n",
        "  sentence=processing(sentence)\r\n",
        "  sentence=transformWords(sentence,vocab)\r\n",
        "  sentence=np.array(sentence)\r\n",
        "  if (len(sentence.shape)==1 and len(sentence)==1):\r\n",
        "    if (model.predict(np.expand_dims(sentence,axis=0))<threshold):\r\n",
        "      commentTypes=\"Negative\"\r\n",
        "    else:\r\n",
        "      commentTypes=\"Positive\"\r\n",
        "  else:\r\n",
        "    commentTypes=[]\r\n",
        "    for i in range(len(sentence)):\r\n",
        "      if (model.predict(np.expand_dims(sentence[i],axis=0))<threshold):\r\n",
        "        commentTypes.append(\"Negative\")\r\n",
        "      else:\r\n",
        "        commentTypes.append(\"Positive\")\r\n",
        "  return commentTypes\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8SXsxKnwSp"
      },
      "source": [
        "<a name=\"7\"></a>\r\n",
        "# Test des modèles sur notre jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkgDTUr4VlfG",
        "outputId": "6a0d8c76-dab1-4073-cce3-6311f0df9ebb"
      },
      "source": [
        "final_LSTM(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee10fe60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee10fe60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee10fe60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Positive', 'Positive', 'Positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNrtuQWTa9Eh",
        "outputId": "6d152c8e-3ef6-4170-8df3-3d961696baa8"
      },
      "source": [
        "final_SimpleRNN(x_test2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee15a050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee15a050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5cee15a050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Positive', 'Positive', 'Positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8WpGTven2zr"
      },
      "source": [
        "<a name=\"8\"></a>\r\n",
        "# Détermination du seuil selon les données observées"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTP2u-38GAeT"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "def defineThreshold(datas,model):\r\n",
        "  y_train=np.concatenate([np.zeros((1000,1)),np.ones((1000,1))])\r\n",
        "  seuils=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\r\n",
        "  F1_score=[]\r\n",
        "  predict=model.predict(datas)\r\n",
        "  for seuil in seuils:\r\n",
        "    predictions=[]\r\n",
        "    for i in range(len(datas)):\r\n",
        "      if (predict[i]<seuil):\r\n",
        "        predictions.append(0)\r\n",
        "      else:\r\n",
        "        predictions.append(1)\r\n",
        "    F1_score.append(f1_score(y_train,predictions))\r\n",
        "  print(F1_score)\r\n",
        "  seuil=seuils[F1_score.index(max(F1_score))]\r\n",
        "  print(seuil)\r\n",
        "  return seuil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuDbula2JVgt"
      },
      "source": [
        "# defineThreshold(np.array(dat),Simple_RNN)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}