{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjetNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antoine723/ProjetNLP/blob/main/ProjetNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HLK8-Mgvzzrg",
        "outputId": "255d8bf8-0b6a-4ab2-8767-e22889842027"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN10vLT_0P2y",
        "outputId": "44785eab-366a-45ea-ac30-30c546dc033b"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzY87idiwDd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3904069-4aa6-4a6a-845f-b58a8649c2ae"
      },
      "source": [
        "df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None)\r\n",
        "data=np.array([df.iloc[0:5][5],df.iloc[800000:800005][5]])\r\n",
        "\r\n",
        "print(data)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
            "  \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"\n",
            "  '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'\n",
            "  'my whole body feels itchy and like its on fire '\n",
            "  \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"]\n",
            " ['I LOVE @Health4UandPets u guys r the best!! '\n",
            "  'im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!'\n",
            "  '@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. '\n",
            "  'Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup'\n",
            "  '@LovesBrooklyn2 he has that effect on everyone ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-hOwDR3sIq"
      },
      "source": [
        "# Traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF6A8FMM47E9"
      },
      "source": [
        "def processing(data):\r\n",
        "\r\n",
        "  #MINUSCULES\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=data[i][j].lower()\r\n",
        "  print(data)\r\n",
        "\r\n",
        "  #TOKENIZATION\r\n",
        "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=tokenizer.tokenize(data[i][j])\r\n",
        "  print(data)\r\n",
        "\r\n",
        "  #STOPWORDS\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[w for w in data[i][j] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "      \r\n",
        "  print(data)\r\n",
        "\r\n",
        "  #STEMMING\r\n",
        "  stemmer=LancasterStemmer()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[stemmer.stem(w) for w in data[i][j]]\r\n",
        "  print(data)\r\n",
        "\r\n",
        "  #LEMMATIZATION\r\n",
        "  Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[Word_Lemmatizer.lemmatize(w) for w in data[i][j]]\r\n",
        "  print(data)\r\n",
        "\r\n",
        "  return data"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGs2Q81z4RKN",
        "outputId": "486ba3b8-bd01-4b2b-f4fb-d4b9ddc73f35"
      },
      "source": [
        "datas=processing(data)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"@switchfoot http://twitpic.com/2y1zl - awww, that's a bummer.  you shoulda got david carr of third day to do it. ;d\"\n",
            "  \"is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!\"\n",
            "  '@kenichan i dived many times for the ball. managed to save 50%  the rest go out of bounds'\n",
            "  'my whole body feels itchy and like its on fire '\n",
            "  \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because i can't see you all over there. \"]\n",
            " ['i love @health4uandpets u guys r the best!! '\n",
            "  'im meeting up with one of my besties tonight! cant wait!!  - girl talk!!'\n",
            "  '@darealsunisakim thanks for the twitter add, sunisa! i got to meet you once at a hin show here in the dc area and you were a sweetheart. '\n",
            "  'being sick can be really cheap when it hurts too much to eat real food  plus, your friends make you soup'\n",
            "  '@lovesbrooklyn2 he has that effect on everyone ']]\n",
            "[[list(['switchfoot', 'http', 'twitpic', 'com', '2y1zl', 'awww', 'that', 's', 'a', 'bummer', 'you', 'shoulda', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', 'd'])\n",
            "  list(['is', 'upset', 'that', 'he', 'can', 't', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', 'blah'])\n",
            "  list(['kenichan', 'i', 'dived', 'many', 'times', 'for', 'the', 'ball', 'managed', 'to', 'save', '50', 'the', 'rest', 'go', 'out', 'of', 'bounds'])\n",
            "  list(['my', 'whole', 'body', 'feels', 'itchy', 'and', 'like', 'its', 'on', 'fire'])\n",
            "  list(['nationwideclass', 'no', 'it', 's', 'not', 'behaving', 'at', 'all', 'i', 'm', 'mad', 'why', 'am', 'i', 'here', 'because', 'i', 'can', 't', 'see', 'you', 'all', 'over', 'there'])]\n",
            " [list(['i', 'love', 'health4uandpets', 'u', 'guys', 'r', 'the', 'best'])\n",
            "  list(['im', 'meeting', 'up', 'with', 'one', 'of', 'my', 'besties', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n",
            "  list(['darealsunisakim', 'thanks', 'for', 'the', 'twitter', 'add', 'sunisa', 'i', 'got', 'to', 'meet', 'you', 'once', 'at', 'a', 'hin', 'show', 'here', 'in', 'the', 'dc', 'area', 'and', 'you', 'were', 'a', 'sweetheart'])\n",
            "  list(['being', 'sick', 'can', 'be', 'really', 'cheap', 'when', 'it', 'hurts', 'too', 'much', 'to', 'eat', 'real', 'food', 'plus', 'your', 'friends', 'make', 'you', 'soup'])\n",
            "  list(['lovesbrooklyn2', 'he', 'has', 'that', 'effect', 'on', 'everyone'])]]\n",
            "[[list(['switchfoot', 'http', 'twitpic', 'com', '2y1zl', 'awww', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day'])\n",
            "  list(['upset', 'update', 'facebook', 'texting', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'])\n",
            "  list(['kenichan', 'dived', 'many', 'times', 'ball', 'managed', 'save', '50', 'rest', 'go', 'bounds'])\n",
            "  list(['whole', 'body', 'feels', 'itchy', 'like', 'fire'])\n",
            "  list(['nationwideclass', 'behaving', 'mad', 'see'])]\n",
            " [list(['love', 'health4uandpets', 'u', 'guys', 'r', 'best'])\n",
            "  list(['im', 'meeting', 'one', 'besties', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n",
            "  list(['darealsunisakim', 'thanks', 'twitter', 'add', 'sunisa', 'got', 'meet', 'hin', 'show', 'dc', 'area', 'sweetheart'])\n",
            "  list(['sick', 'really', 'cheap', 'hurts', 'much', 'eat', 'real', 'food', 'plus', 'friends', 'make', 'soup'])\n",
            "  list(['lovesbrooklyn2', 'effect', 'everyone'])]]\n",
            "[[list(['switchfoot', 'http', 'twitp', 'com', '2y1zl', 'awww', 'bum', 'should', 'got', 'david', 'car', 'third', 'day'])\n",
            "  list(['upset', 'upd', 'facebook', 'text', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'])\n",
            "  list(['kenich', 'div', 'many', 'tim', 'bal', 'man', 'sav', '50', 'rest', 'go', 'bound'])\n",
            "  list(['whol', 'body', 'feel', 'itchy', 'lik', 'fir'])\n",
            "  list(['nationwideclass', 'behav', 'mad', 'see'])]\n",
            " [list(['lov', 'health4uandpets', 'u', 'guy', 'r', 'best'])\n",
            "  list(['im', 'meet', 'on', 'besty', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n",
            "  list(['darealsunisakim', 'thank', 'twit', 'ad', 'sunis', 'got', 'meet', 'hin', 'show', 'dc', 'are', 'sweetheart'])\n",
            "  list(['sick', 'real', 'cheap', 'hurt', 'much', 'eat', 'real', 'food', 'plu', 'friend', 'mak', 'soup'])\n",
            "  list(['lovesbrooklyn2', 'effect', 'everyon'])]]\n",
            "[[list(['switchfoot', 'http', 'twitp', 'com', '2y1zl', 'awww', 'bum', 'should', 'got', 'david', 'car', 'third', 'day'])\n",
            "  list(['upset', 'upd', 'facebook', 'text', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'])\n",
            "  list(['kenich', 'div', 'many', 'tim', 'bal', 'man', 'sav', '50', 'rest', 'go', 'bound'])\n",
            "  list(['whol', 'body', 'feel', 'itchy', 'lik', 'fir'])\n",
            "  list(['nationwideclass', 'behav', 'mad', 'see'])]\n",
            " [list(['lov', 'health4uandpets', 'u', 'guy', 'r', 'best'])\n",
            "  list(['im', 'meet', 'on', 'besty', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n",
            "  list(['darealsunisakim', 'thank', 'twit', 'ad', 'sunis', 'got', 'meet', 'hin', 'show', 'dc', 'are', 'sweetheart'])\n",
            "  list(['sick', 'real', 'cheap', 'hurt', 'much', 'eat', 'real', 'food', 'plu', 'friend', 'mak', 'soup'])\n",
            "  list(['lovesbrooklyn2', 'effect', 'everyon'])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW3eO84_KFBt"
      },
      "source": [
        "def rnn_forward(x, a0, parameters):\r\n",
        "    \"\"\"\r\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\r\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\r\n",
        "    parameters -- python dictionary containing:\r\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\r\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\r\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\r\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\r\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\r\n",
        "\r\n",
        "    Returns:\r\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\r\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\r\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    # Initialize \"caches\" which will contain the list of all caches\r\n",
        "    caches = []\r\n",
        "    \r\n",
        "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\r\n",
        "    n_x, m, T_x = x.shape\r\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\r\n",
        "    \r\n",
        "    ### START CODE HERE ###\r\n",
        "    \r\n",
        "    # initialize \"a\" and \"y\" with zeros (≈2 lines)\r\n",
        "    a = np.zeros((n_a,m,T_x))\r\n",
        "    y_pred = np.zeros((n_y,m,T_x))\r\n",
        "    \r\n",
        "    # Initialize a_next (≈1 line)\r\n",
        "    a_next = a0\r\n",
        "    \r\n",
        "    # loop over all time-steps\r\n",
        "    for t in range(T_x):\r\n",
        "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\r\n",
        "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\r\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\r\n",
        "        a[:,:,t] = a_next\r\n",
        "        # Save the value of the prediction in y (≈1 line)\r\n",
        "        y_pred[:,:,t] = yt_pred\r\n",
        "        # Append \"cache\" to \"caches\" (≈1 line)\r\n",
        "        caches.append(cache)\r\n",
        "        \r\n",
        "    ### END CODE HERE ###\r\n",
        "    \r\n",
        "    # store values needed for backward propagation in cache\r\n",
        "    caches = (caches, x)\r\n",
        "    \r\n",
        "    return a, y_pred, caches"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "8JhSXa4KKHQ7",
        "outputId": "ea342bb1-5974-481d-ff27-17f496e2ccbc"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "x = np.random.randn(3,10,4)\r\n",
        "a0 = np.random.randn(5,10)\r\n",
        "Waa = np.random.randn(5,5)\r\n",
        "Wax = np.random.randn(5,3)\r\n",
        "Wya = np.random.randn(2,5)\r\n",
        "ba = np.random.randn(5,1)\r\n",
        "by = np.random.randn(2,1)\r\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\r\n",
        "\r\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\r\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-cfd31ba695fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Waa\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wax\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wya\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ba\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"by\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-153-e65bef16eef7>\u001b[0m in \u001b[0;36mrnn_forward\u001b[0;34m(x, a0, parameters)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Update next hidden state, compute the prediction, get the cache (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_cell_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Save the value of the new \"next\" hidden state in a (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-8dcdf9ea5e6e>\u001b[0m in \u001b[0;36mrnn_cell_forward\u001b[0;34m(xt, a_prev, parameters)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# compute output of the current cell using the formula given above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0myt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6eAwEM8HIB"
      },
      "source": [
        "model=tf.keras.Sequential()\r\n",
        "model.add(tf.keras.layers.Embedding(5,2))\r\n",
        "model.compile('rmsprop', 'mse')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyL0yeQLJeUW"
      },
      "source": [
        ""
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-eZRODyJy1x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}