{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjetNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antoine723/ProjetNLP/blob/main/ProjetNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLK8-Mgvzzrg",
        "outputId": "8fca9f83-7d71-40df-b88c-bf58bcbdb5b6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN10vLT_0P2y",
        "outputId": "c5ab7830-746c-4065-dc11-d2efb089051f"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from gdrive.MyDrive.ProjetNLP.rnn_utils import *\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzY87idiwDd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585873ac-9346-4876-95e6-5960e4fdec61"
      },
      "source": [
        "df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None)\r\n",
        "data=np.array([df.iloc[0:15][5],df.iloc[800000:800015][5]])\r\n",
        "\r\n",
        "print(data)"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
            "  \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"\n",
            "  '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'\n",
            "  'my whole body feels itchy and like its on fire '\n",
            "  \"@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \"\n",
            "  '@Kwesidei not the whole crew ' 'Need a hug '\n",
            "  \"@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\"\n",
            "  \"@Tatiana_K nope they didn't have it \" '@twittera que me muera ? '\n",
            "  \"spring break in plain city... it's snowing \"\n",
            "  'I just re-pierced my ears '\n",
            "  \"@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\"\n",
            "  '@octolinz16 It it counts, idk why I did either. you never talk to me anymore '\n",
            "  \"@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\"]\n",
            " ['I LOVE @Health4UandPets u guys r the best!! '\n",
            "  'im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!'\n",
            "  '@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. '\n",
            "  'Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup'\n",
            "  '@LovesBrooklyn2 he has that effect on everyone '\n",
            "  '@ProductOfFear You can tell him that I just burst out laughing really loud because of that  Thanks for making me come out of my sulk!'\n",
            "  '@r_keith_hill Thans for your response. Ihad already find this answer '\n",
            "  \"@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!! \"\n",
            "  '@tommcfly ah, congrats mr fletcher for finally joining twitter '\n",
            "  '@e4VoIP I RESPONDED  Stupid cat is helping me type. Forgive errors '\n",
            "  'crazy day of school. there for 10 hours straiiight. about to watch the hills. @spencerpratt told me too! ha. happy birthday JB! '\n",
            "  '@naughtyhaughty HOW DID I FORGET ABOUT TWO AND A HALF MEN?!?!? I LOVE THAT SHOW!!! '\n",
            "  \"@nileyjileyluver Haha, don't worry! You'll get the hang of it! \"\n",
            "  \"@soundwav2010 At least I won't be the only one feeling lost! This may cause me many later than usual nights, already addicting \"\n",
            "  \"@LutheranLucciol Make sure you DM me if you post a link to that video! &lt;LOL&gt;So I don't miss it   Better get permission and blessing first?\"]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-hOwDR3sIq"
      },
      "source": [
        "# Traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF6A8FMM47E9"
      },
      "source": [
        "def processing(data):\r\n",
        "\r\n",
        "  #MINUSCULES\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=data[i][j].lower()\r\n",
        "\r\n",
        "  #TOKENIZATION\r\n",
        "  tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=tokenizer.tokenize(data[i][j])\r\n",
        "\r\n",
        "  #STOPWORDS\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[w for w in data[i][j] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "      \r\n",
        "\r\n",
        "  #STEMMING\r\n",
        "  stemmer=LancasterStemmer()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[stemmer.stem(w) for w in data[i][j]]\r\n",
        "\r\n",
        "  #LEMMATIZATION\r\n",
        "  Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      data[i][j]=[Word_Lemmatizer.lemmatize(w) for w in data[i][j]]\r\n",
        "  print(data)\r\n",
        "\r\n",
        "  return data"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pLWssXU_PxP",
        "outputId": "9e8d8481-983a-49a7-a2c4-1f557db747d6"
      },
      "source": [
        "data=processing(data)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[list(['switchfoot', 'http', 'twitp', 'com', '2y1zl', 'awww', 'bum', 'should', 'got', 'david', 'car', 'third', 'day'])\n",
            "  list(['upset', 'upd', 'facebook', 'text', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'])\n",
            "  list(['kenich', 'div', 'many', 'tim', 'bal', 'man', 'sav', '50', 'rest', 'go', 'bound'])\n",
            "  list(['whol', 'body', 'feel', 'itchy', 'lik', 'fir'])\n",
            "  list(['nationwideclass', 'behav', 'mad', 'see'])\n",
            "  list(['kweside', 'whol', 'crew']) list(['nee', 'hug'])\n",
            "  list(['lolt', 'hey', 'long', 'tim', 'see', 'ye', 'rain', 'bit', 'bit', 'lol', 'fin', 'thank'])\n",
            "  list(['tatiana_k', 'nop']) list(['twitter', 'que', 'muer'])\n",
            "  list(['spring', 'break', 'plain', 'city', 'snow'])\n",
            "  list(['pierc', 'ear'])\n",
            "  list(['careg', 'bear', 'watch', 'thought', 'ua', 'loss', 'embarrass'])\n",
            "  list(['octolinz16', 'count', 'idk', 'eith', 'nev', 'talk', 'anym'])\n",
            "  list(['smarrison', 'would', 'first', 'gun', 'real', 'though', 'zac', 'snyd', 'doucheclown'])]\n",
            " [list(['lov', 'health4uandpets', 'u', 'guy', 'r', 'best'])\n",
            "  list(['im', 'meet', 'on', 'besty', 'tonight', 'cant', 'wait', 'girl', 'talk'])\n",
            "  list(['darealsunisakim', 'thank', 'twit', 'ad', 'sunis', 'got', 'meet', 'hin', 'show', 'dc', 'are', 'sweetheart'])\n",
            "  list(['sick', 'real', 'cheap', 'hurt', 'much', 'eat', 'real', 'food', 'plu', 'friend', 'mak', 'soup'])\n",
            "  list(['lovesbrooklyn2', 'effect', 'everyon'])\n",
            "  list(['productoffear', 'tel', 'burst', 'laugh', 'real', 'loud', 'thank', 'mak', 'com', 'sulk'])\n",
            "  list(['r_keith_hill', 'than', 'respons', 'ihad', 'already', 'find', 'answ'])\n",
            "  list(['keepinupwkr', 'jeal', 'hop', 'gre', 'tim', 'vega', 'lik', 'acm', 'lov', 'show'])\n",
            "  list(['tommcf', 'ah', 'congr', 'mr', 'fletch', 'fin', 'join', 'twit'])\n",
            "  list(['e4voip', 'respond', 'stupid', 'cat', 'help', 'typ', 'forg', 'er'])\n",
            "  list(['crazy', 'day', 'school', '10', 'hour', 'straiiight', 'watch', 'hil', 'spencerprat', 'told', 'ha', 'happy', 'birthday', 'jb'])\n",
            "  list(['naughtyhaughty', 'forget', 'two', 'half', 'men', 'lov', 'show'])\n",
            "  list(['nileyjileyluv', 'hah', 'worry', 'get', 'hang'])\n",
            "  list(['soundwav2010', 'least', 'on', 'feel', 'lost', 'may', 'caus', 'many', 'lat', 'u', 'night', 'already', 'addict'])\n",
            "  list(['lutheranlucciol', 'mak', 'sur', 'dm', 'post', 'link', 'video', 'lt', 'lol', 'gt', 'miss', 'bet', 'get', 'permit', 'bless', 'first'])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykywsQVzQfCE",
        "outputId": "7520c09a-d04d-4f7c-a83d-4c1d18960938"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "v=set()\r\n",
        "for i in range(len(data)):\r\n",
        "  for j in range(len(data[i])):\r\n",
        "    for h in range(len(data[i][j])):\r\n",
        "      v.add(data[i][j][h])\r\n",
        "\r\n",
        "vocab = defaultdict(lambda: 0)\r\n",
        "vocab['<PAD>'] = 1\r\n",
        "\r\n",
        "j=2\r\n",
        "for i in v:\r\n",
        "  vocab[i]=j\r\n",
        "  j+=1\r\n",
        "print(vocab)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function <lambda> at 0x7f365d0cf1e0>, {'<PAD>': 1, 'snow': 2, 'smarrison': 3, 'que': 4, 'break': 5, 'ua': 6, 'are': 7, 'dm': 8, 'addict': 9, 'bear': 10, 'bless': 11, 'embarrass': 12, 'answ': 13, 'friend': 14, 'lost': 15, 'long': 16, 'bet': 17, 'anym': 18, 'careg': 19, 'pierc': 20, 'hah': 21, 'blah': 22, 'go': 23, 'also': 24, 'hang': 25, 'hour': 26, 'two': 27, 'lovesbrooklyn2': 28, 'sweetheart': 29, 'tonight': 30, 'forget': 31, 'mr': 32, 'com': 33, 'talk': 34, 'would': 35, 'stupid': 36, 'see': 37, 'im': 38, 'sulk': 39, 'on': 40, 'keepinupwkr': 41, 'fir': 42, 'birthday': 43, 'nationwideclass': 44, 'nileyjileyluv': 45, 'city': 46, 'food': 47, 'soup': 48, 'rest': 49, 'find': 50, 'lol': 51, 'rain': 52, 'tim': 53, 'hurt': 54, 'zac': 55, 'show': 56, 'lik': 57, 'gun': 58, 'man': 59, 'effect': 60, 'lt': 61, 'join': 62, 'ear': 63, 'congr': 64, 'thank': 65, 'respons': 66, 'watch': 67, 'result': 68, 'snyd': 69, 'ad': 70, 'kenich': 71, 'behav': 72, 'ye': 73, 'loss': 74, 'r': 75, 'gre': 76, 'loud': 77, 'upd': 78, 'might': 79, 'guy': 80, 'sick': 81, 'productoffear': 82, 'hop': 83, 'told': 84, 'switchfoot': 85, 'cat': 86, 'body': 87, '2y1zl': 88, 'first': 89, 'octolinz16': 90, 'post': 91, 'mad': 92, 'school': 93, 'cheap': 94, 'forg': 95, 'ha': 96, 'many': 97, 'feel': 98, 'best': 99, 'crazy': 100, 'though': 101, 'awww': 102, 'day': 103, 'hin': 104, 'sur': 105, 'spring': 106, 'lolt': 107, 'dc': 108, 'bum': 109, 'should': 110, 'plu': 111, 'ihad': 112, 'muer': 113, 'twitp': 114, 'fin': 115, 'itchy': 116, 'half': 117, 'link': 118, 'caus': 119, 'jeal': 120, 'wait': 121, 'tommcf': 122, 'hey': 123, 'sunis': 124, 'get': 125, 'eat': 126, 'tatiana_k': 127, 'lat': 128, 'text': 129, 'happy': 130, 'kweside': 131, 'night': 132, 'least': 133, 'everyon': 134, 'crew': 135, 'may': 136, 'straiiight': 137, 'miss': 138, 'count': 139, 'typ': 140, 'cry': 141, 'ah': 142, 'er': 143, 'got': 144, 'acm': 145, 'doucheclown': 146, 'today': 147, 'real': 148, '10': 149, 'thought': 150, 'car': 151, 'laugh': 152, 'facebook': 153, 'besty': 154, 'lov': 155, 'third': 156, 'bal': 157, 'naughtyhaughty': 158, 'respond': 159, 'hil': 160, 'sav': 161, 'david': 162, 'nee': 163, 'bit': 164, 'idk': 165, 'health4uandpets': 166, 'cant': 167, 'e4voip': 168, 'soundwav2010': 169, 'worry': 170, 'meet': 171, 'than': 172, 'bound': 173, 'upset': 174, 'spencerprat': 175, 'already': 176, 'r_keith_hill': 177, 'vega': 178, 'tel': 179, 'http': 180, 'jb': 181, 'div': 182, 'mak': 183, 'girl': 184, 'fletch': 185, 'gt': 186, 'permit': 187, 'twit': 188, 'darealsunisakim': 189, 'burst': 190, 'whol': 191, 'nev': 192, 'much': 193, 'u': 194, 'plain': 195, 'video': 196, 'help': 197, 'hug': 198, 'eith': 199, 'men': 200, 'nop': 201, 'lutheranlucciol': 202, 'twitter': 203, '50': 204})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9z0zbQbO31u"
      },
      "source": [
        "def transformWords(data):\r\n",
        "  matrixData=[]\r\n",
        "  maxLen=0\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      if (maxLen < len(data[i][j])):\r\n",
        "        maxLen= len(data[i][j])\r\n",
        "\r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      test=[]\r\n",
        "      for k in range(maxLen):\r\n",
        "        if (k >= len(data[i][j])):\r\n",
        "          test.append(vocab['<PAD>'])\r\n",
        "        else:\r\n",
        "          test.append(vocab[data[i][j][k]])\r\n",
        "      matrixData.append(test)\r\n",
        "  return matrixData"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyL0yeQLJeUW",
        "outputId": "35b120c6-f71e-410c-da4a-285ab832e995"
      },
      "source": [
        "data=transformWords(data)\r\n",
        "datas=np.array([df.iloc[5:10][5],df.iloc[800005:800010][5]])\r\n",
        "datas=processing(datas)\r\n",
        "datas=transformWords(datas)"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[list(['kweside', 'whol', 'crew']) list(['nee', 'hug'])\n",
            "  list(['lolt', 'hey', 'long', 'tim', 'see', 'ye', 'rain', 'bit', 'bit', 'lol', 'fin', 'thank'])\n",
            "  list(['tatiana_k', 'nop']) list(['twitter', 'que', 'muer'])]\n",
            " [list(['productoffear', 'tel', 'burst', 'laugh', 'real', 'loud', 'thank', 'mak', 'com', 'sulk'])\n",
            "  list(['r_keith_hill', 'than', 'respons', 'ihad', 'already', 'find', 'answ'])\n",
            "  list(['keepinupwkr', 'jeal', 'hop', 'gre', 'tim', 'vega', 'lik', 'acm', 'lov', 'show'])\n",
            "  list(['tommcf', 'ah', 'congr', 'mr', 'fletch', 'fin', 'join', 'twit'])\n",
            "  list(['e4voip', 'respond', 'stupid', 'cat', 'help', 'typ', 'forg', 'er'])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY5YAQgF-ybW"
      },
      "source": [
        "data=np.array(data)"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6eAwEM8HIB"
      },
      "source": [
        "def buildModel():\r\n",
        "  model=tf.keras.Sequential()\r\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=1))\r\n",
        "  model.add(tf.keras.layers.LSTM(128,activation=\"relu\"))\r\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n",
        "  return model"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFaMVFSxZHcH",
        "outputId": "f2a18913-3b47-4307-f72c-ff8ac59287e2"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 85 180 114  33  88 102 109 110 144 162 151 156 103   1   1   1]\n",
            " [174  78 153 129  79 141  68  93 147  24  22   1   1   1   1   1]\n",
            " [ 71 182  97  53 157  59 161 204  49  23 173   1   1   1   1   1]\n",
            " [191  87  98 116  57  42   1   1   1   1   1   1   1   1   1   1]\n",
            " [ 44  72  92  37   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [131 191 135   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [163 198   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [107 123  16  53  37  73  52 164 164  51 115  65   1   1   1   1]\n",
            " [127 201   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [203   4 113   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [106   5 195  46   2   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [ 20  63   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [ 19  10  67 150   6  74  12   1   1   1   1   1   1   1   1   1]\n",
            " [ 90 139 165 199 192  34  18   1   1   1   1   1   1   1   1   1]\n",
            " [  3  35  89  58 148 101  55  69 146   1   1   1   1   1   1   1]\n",
            " [155 166 194  80  75  99   1   1   1   1   1   1   1   1   1   1]\n",
            " [ 38 171  40 154  30 167 121 184  34   1   1   1   1   1   1   1]\n",
            " [189  65 188  70 124 144 171 104  56 108   7  29   1   1   1   1]\n",
            " [ 81 148  94  54 193 126 148  47 111  14 183  48   1   1   1   1]\n",
            " [ 28  60 134   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [ 82 179 190 152 148  77  65 183  33  39   1   1   1   1   1   1]\n",
            " [177 172  66 112 176  50  13   1   1   1   1   1   1   1   1   1]\n",
            " [ 41 120  83  76  53 178  57 145 155  56   1   1   1   1   1   1]\n",
            " [122 142  64  32 185 115  62 188   1   1   1   1   1   1   1   1]\n",
            " [168 159  36  86 197 140  95 143   1   1   1   1   1   1   1   1]\n",
            " [100 103  93 149  26 137  67 160 175  84  96 130  43 181   1   1]\n",
            " [158  31  27 117 200 155  56   1   1   1   1   1   1   1   1   1]\n",
            " [ 45  21 170 125  25   1   1   1   1   1   1   1   1   1   1   1]\n",
            " [169 133  40  98  15 136 119  97 128 194 132 176   9   1   1   1]\n",
            " [202 183 105   8  91 118 196  61  51 186 138  17 125 187  11  89]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1tAB7_Sugnx"
      },
      "source": [
        "# print(y_train)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZJHW75PqsU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1fe932-209b-49c4-a7e9-dedc576703a1"
      },
      "source": [
        "model=buildModel()\r\n",
        "# def fitModel(model,trainSize): #trainSize = taille des données prises en compte pour le fit (ex : si on crée le modèle en lui donnant 20 phrases, trainSize=20)\r\n",
        "#Attention, ici on a configuré le y_train pour qu'il y ait une part égale de commentaires positifs/négatifs\r\n",
        "y_train=np.concatenate([np.zeros((15,1)),np.ones((15,1))])\r\n",
        "model.compile(loss='binary_crossentropy',optimizer=\"adam\")\r\n",
        "model.fit(data,y_train,epochs=50)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6931\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6929\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6927\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6926\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6925\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6923\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6920\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6918\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6914\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6909\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6903\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6895\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6885\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6871\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6854\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6833\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6805\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6769\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6723\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6664\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6590\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6496\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6383\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6249\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6091\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5904\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5677\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5399\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5054\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4633\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4130\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3534\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2825\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2118\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1410\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0859\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0384\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0154\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0030\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 6.4770e-04\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 7.7134e-05\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.8147e-06\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 2.3972e-09\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 3.7482e-14\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 2.7151e-21\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 7.0303e-31\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3646998438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKB4t_HsvYWt",
        "outputId": "cab3efb1-1811-4404-8ee8-d65205a6a7c0"
      },
      "source": [
        "model.predict(datas)"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3663570158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.10714380e-08],\n",
              "       [2.34102657e-08],\n",
              "       [1.05887485e-08],\n",
              "       [1.29744659e-09],\n",
              "       [3.56525183e-04],\n",
              "       [1.00000000e+00],\n",
              "       [1.00000000e+00],\n",
              "       [1.00000000e+00],\n",
              "       [1.00000000e+00],\n",
              "       [1.00000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPOC8riZvdsp",
        "outputId": "197b24e4-3987-41db-f49d-ef6be65a5670"
      },
      "source": [
        "print(data[0].shape)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF_zV6RD41uw",
        "outputId": "58b7a304-1713-4faf-945c-4199df894502"
      },
      "source": [
        "print(len(df))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}