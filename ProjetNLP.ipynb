{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjetNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antoine723/ProjetNLP/blob/main/ProjetNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZNE_eBooDgL"
      },
      "source": [
        "- [Import bibliothèques et fichiers utiles](#0)\r\n",
        "- [Récupération commentaires API Yelp](#1)\r\n",
        "- [Traitement](#2)\r\n",
        "- [Création vocabulaire grâce aux données](#3)\r\n",
        "- [Transformation des mots en indices](#4)\r\n",
        "- [Création modèles](#5)\r\n",
        "- [Pipeline](#6)\r\n",
        "- [Test des modèles sur notre jeu de données](#7)\r\n",
        "- [Détermination du seuil selon les données observées](#8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz0GoIXymh0r"
      },
      "source": [
        "<a name=\"0\"></a>\r\n",
        "# Import bibliothèques et fichiers utiles\r\n",
        "\r\n",
        "#####Import des bibliothèques nécessaires pour la tokenization, stemming, lemmatization, des bibliothèques nécessaires pour le deep learning et autres (numpy, pandas...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLK8-Mgvzzrg",
        "outputId": "048df44b-6b69-4d3a-911f-471404641b73"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN10vLT_0P2y",
        "outputId": "0e2ca1dc-94b1-4761-b4fb-3677ff8a242b"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from gdrive.MyDrive.ProjetNLP.rnn_utils import *\r\n",
        "\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSV85DFHuEVj",
        "outputId": "5b6486ac-14a3-402e-f0b3-f85df4a40b0a"
      },
      "source": [
        "pip install --pre gql[all]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gql[all] in /usr/local/lib/python3.7/dist-packages (3.0.0a5)\n",
            "Requirement already satisfied: graphql-core<3.2,>=3.1 in /usr/local/lib/python3.7/dist-packages (from gql[all]) (3.1.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.7/dist-packages (from gql[all]) (1.6.3)\n",
            "Requirement already satisfied: requests<3,>=2.23; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gql[all]) (2.23.0)\n",
            "Requirement already satisfied: websockets<9,>=8.1; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gql[all]) (8.1)\n",
            "Requirement already satisfied: aiohttp<3.8.0,>=3.7.1; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gql[all]) (3.7.4)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.6->gql[all]) (5.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.6->gql[all]) (3.7.4.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.6->gql[all]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.23; extra == \"all\"->gql[all]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.23; extra == \"all\"->gql[all]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.23; extra == \"all\"->gql[all]) (2020.12.5)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.7.1; extra == \"all\"->gql[all]) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<3.8.0,>=3.7.1; extra == \"all\"->gql[all]) (20.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyr7Wo3-w3F2"
      },
      "source": [
        "import requests\r\n",
        "import json"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObuheqgcmuOs"
      },
      "source": [
        "<a name=\"1\"></a>\r\n",
        "# Récupération commentaires API Yelp\r\n",
        "\r\n",
        "#####On récupère les commentaires qui nous permettront de  tester notre pipeline via l'API Yelp (n'en transmet que 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caekNUL8uI2-",
        "outputId": "536f88cf-f49a-45c9-9ab3-fc59d693970d"
      },
      "source": [
        "api_key='MI6JGndpuW3Kyc9cIjj5of9AhZUKET4JlGdRsiBnGZjL2ofbChjG0ksWdjXrHvdRN3cHosvu6bkDcvdOUCc5A_m-IAciq5kpjSqAxt5euSYLzB0q0vvWqmuyrI86YHYx'\r\n",
        "#Clé API nécessaire pour l'utilisation de l'API Yelp (propre à chaque utilisateur)\r\n",
        "headers = {'Authorization': 'Bearer %s' % api_key}\r\n",
        "\r\n",
        "url = \"https://api.yelp.com/v3/businesses/FEVQpbOPOwAPNIgO7D3xxw/reviews\"\r\n",
        "req = requests.get(url, headers=headers)\r\n",
        "\r\n",
        "dataset=json.loads(req.text)[\"reviews\"] #On récupère les commentaires fournis par l'API Yelp\r\n",
        "x_test=[]\r\n",
        "for i in range(len(dataset)):\r\n",
        "  x_test.append(dataset[i][\"text\"])\r\n",
        "x_test2=x_test.copy() #On a créé 2 variables pour tester nos 2 modèles\r\n",
        "print(x_test)\r\n",
        "print(x_test2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\", 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...', 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...']\n",
            "[\"Truth be told if it was up to me I'd be giving 4/5 stars, we did order recently and our cheese fries weren't delivered, couldn't speak with anyone from...\", 'Happened to be in the city today and not too far from here so I had to stop by and pick up cheese fries (I love those crinkle cuts!) and a shake. Yum. Tbh,...', 'Before I moved to NY, my first ever trip to the city was in 2005 - this was when Shake Shack had this one lone original location, and it was an event to go...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXOaSu6oLSkq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-hOwDR3sIq"
      },
      "source": [
        "<a name=\"2\"></a>\r\n",
        "# Traitement\r\n",
        "\r\n",
        "#####Mise en minuscule de tous les mots, tokenization, suppression des stopwords, stemming et lemmatization, la fonction prend en entrée la donnée à traiter et renvoie la donnée après traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF6A8FMM47E9"
      },
      "source": [
        "def processing(data): #Fonction qui effectue tous le traitement nécessaire (minuscules, tokenization, suppression des stopwords, stemming et lemmatization)\r\n",
        "  if (isinstance(data,str)): #Pour permettre de tester nos modèles sur une phrase brute et non pas un tablea, on vérifie le type de données que l'on a en entrée\r\n",
        "#Si la donnée est une chaîne de caractère on va effectuer le traitement sur celle-ci\r\n",
        "    #MINUSCULES\r\n",
        "    data=data.lower()\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "    data=tokenizer.tokenize(data)\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    data=[w for w in data if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    data=[stemmer.stem(w) for w in data]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    data=[Word_Lemmatizer.lemmatize(w) for w in data]\r\n",
        "\r\n",
        "  elif (isinstance(data,list)): #Si la donnée est une liste (pour tester nos modèles sur une liste de phrases)\r\n",
        "    for i in range(len(data)): #On parcourt juste chaque élément de cette liste à chaque étape du traitement pour faire le même traitement que précédemment, mais\r\n",
        "    #sur chacune des phrases indépendamment\r\n",
        "      data[i]=data[i].lower()\r\n",
        "\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=tokenizer.tokenize(data[i])\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[w for w in data[i] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "        \r\n",
        "\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[stemmer.stem(w) for w in data[i]]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      data[i]=[Word_Lemmatizer.lemmatize(w) for w in data[i]]\r\n",
        "\r\n",
        "  else : #Enfin, cette condition nous permet de traiter nos données pour l'entraînement de nos modèles, la forme de ces données était légèrement différente car\r\n",
        "  #plusieurs dimensions : liste de 2 listes (positif/négatif)\r\n",
        "  #Le traitement est le même mais on a 2 boucles for imbriquées\r\n",
        "    #MINUSCULES\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=data[i][j].lower()\r\n",
        "\r\n",
        "    #TOKENIZATION\r\n",
        "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=tokenizer.tokenize(data[i][j])\r\n",
        "\r\n",
        "    #STOPWORDS\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[w for w in data[i][j] if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "        \r\n",
        "\r\n",
        "    #STEMMING\r\n",
        "    stemmer=LancasterStemmer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[stemmer.stem(w) for w in data[i][j]]\r\n",
        "\r\n",
        "    #LEMMATIZATION\r\n",
        "    Word_Lemmatizer = WordNetLemmatizer()\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        data[i][j]=[Word_Lemmatizer.lemmatize(w) for w in data[i][j]]\r\n",
        "\r\n",
        "  return data #On renvoie la donnée traitée"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDjBDJZMm1_G"
      },
      "source": [
        "<a name=\"3\"></a>\r\n",
        "# Création du vocabulaire grâce aux données\r\n",
        "\r\n",
        "#####On crée notre vocabulaire nécessaire en tant que dictionnaire, qui contient comme clés l'ensemble des mots de notre dataset et comme valeurs leurs indices. Il contient une clé-valeur spécifique pour le padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykywsQVzQfCE"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "def defineVocab(data): #Fonction qui définit le vocabulaire à partir des données passées en entrée\r\n",
        "  v=set() #On crée un set qui est très utile pour déterminer le vocabulaire car il est rempli sans avoir de doublons \r\n",
        "  for i in range(len(data)):\r\n",
        "    for j in range(len(data[i])):\r\n",
        "      for h in range(len(data[i][j])): #On parcourt tous les MOTS de notre jeu de données (ce qui explique les 3 boucles for imbriquées)\r\n",
        "        v.add(data[i][j][h]) #Comme expliqué précédemment, on ajoute le mot au set, mais celui-ci ne va être ajouté que s'il n'est pas déjà présent\r\n",
        "\r\n",
        "  vocab = defaultdict(lambda: 0) #On initialise le dictionnaire de telle manière que pour toute valeur non présente en tant que clé dans celui-ci, la valeur attribuée sera ici 0\r\n",
        "  #Autrement dit, lorsqu'un mot ne sera pas présent dans le dictionnaire (lors de la prédiction), son indice correspondant sera \"0\"\r\n",
        "  vocab['<PAD>'] = 1 #Comme on va effectuer du padding, on définit une valeur pour du padding : la valeur 1\r\n",
        "\r\n",
        "  j=2 #On commence donc l'initialisation d'indices à partir de la valeur 2\r\n",
        "  for i in v: #Ici, on va donc remplir le dictionnaire (vocabulaire), pour chacun des mots du set avec un indice quelconque\r\n",
        "    vocab[i]=j\r\n",
        "    j+=1\r\n",
        "  return vocab"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTWcxxdtnBKU"
      },
      "source": [
        "<a name=\"4\"></a>\r\n",
        "# Transformation des mots en indices\r\n",
        "\r\n",
        "#####A l'aide du vocabulaire déterminé précédemment, on transforme les mots en leurs indices correspondants. Si le mot est dans le vocabulaire, on le change en son indice, s'il est nécessaire de faire du padding, il prendra son indice également, en revanche, s'il n'est pas dans le vocabulaire, il prendra la valeur \"0\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9z0zbQbO31u"
      },
      "source": [
        "def transformWords(data,vocab): #Fonction qui va transformer les mots en leur indice correspondant dans le vocabulaire\r\n",
        "  matrixData=[]\r\n",
        "  maxLen=0 #variable qui va stocker la taille de la plus grande phrase du jeu de données\r\n",
        "  if (isinstance(data,list)): #De la même manière que pour la fonction de traitement, on a adapté selon la forme de la donnée\r\n",
        "  #Ici, on teste si la donnée en entrée est une liste, cette condition est respectée lors de la prédiction, que celle-ci soit effectuée pour une phrase brute \r\n",
        "  #(car elle est transformée en liste de mots après le traitement) ou pour une liste de phrases\r\n",
        "    if (len(data)>1): #On distingue justement ces 2 cas, si c'est une liste de phrases, la taille de la liste sera supérieure à 1 \r\n",
        "      for i in range(len(data)):\r\n",
        "        tmp=[] #On initialise une liste tmp qui va contenir les indices des mots transformés, et ce pour chacune des phrases\r\n",
        "        for j in data[i]:\r\n",
        "          if (j not in vocab.keys()): #Comme expliqué précédemment, si le mot de la prédiction n'est pas dans le vocabulaire, il prend la valeur 0, ici on ne procède pas comme\r\n",
        "          #décrit précédemment car le vocabulaire ici a été chargé depuis le fichier CSV et n'est donc plus un defaultdict\r\n",
        "            tmp.append(0)\r\n",
        "          else: #Sinon, on ajoute l'indice correspondant au mot\r\n",
        "            tmp.append(vocab[j])\r\n",
        "        matrixData.append(tmp) #On ajoute la liste d'indices au fur et à mesure dans la liste finale\r\n",
        "    else: #Sinon elle sera de 1\r\n",
        "      for i in data:\r\n",
        "        if (i not in vocab.keys()):\r\n",
        "          matrixData.append(0)\r\n",
        "        else:\r\n",
        "          matrixData.append(vocab[i])\r\n",
        "  else: #L'autre condition est respectée uniquement lors de l'entraînement car c'est un numpy array\r\n",
        "    for i in range(len(data)): #Le traitement est le même, à l'exception qu'ici on doit faire du padding donc on doit d'abord récupéré la taille maximale\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        if (maxLen < len(data[i][j])):\r\n",
        "          maxLen= len(data[i][j])\r\n",
        "\r\n",
        "    for i in range(len(data)):\r\n",
        "      for j in range(len(data[i])):\r\n",
        "        tmp=[]\r\n",
        "        for k in range(maxLen):\r\n",
        "          if (k >= len(data[i][j])): #Si l'on a atteint la fin de la phrase mais qu'elle doit subir du padding, on lui ajoute les indices correspondants jusqu'à avoir\r\n",
        "          #la taille requise\r\n",
        "            tmp.append(vocab['<PAD>'])\r\n",
        "          else:\r\n",
        "            tmp.append(vocab[data[i][j][k]])\r\n",
        "        matrixData.append(tmp)\r\n",
        "  return matrixData"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpSs72yjnM8u"
      },
      "source": [
        "<a name=\"5\"></a>\r\n",
        "# Création modèles\r\n",
        "\r\n",
        "#####On crée nos 2 modèles pour notre baseline : les 2 sont basés sur du RNN, le 1er étant du LSTM, l'autre du simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6eAwEM8HIB"
      },
      "source": [
        "def buildModelLSTM(vocab): #On construit notre modèle LSTM\r\n",
        "  model=tf.keras.Sequential()\r\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=1)) #On ajoute une couche d'embedding en prenant en entrée la taille du vocabulaire \r\n",
        "  #(+1 sinon l'entraînement ne s'exécute pas )\r\n",
        "  model.add(tf.keras.layers.LSTM(128,activation=\"relu\")) #On ajoute une couche LSTM avec la fonction d'activation relu\r\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))  #On ajoute une couche finale avec la fonction d'activation sigmoid\r\n",
        "  return model\r\n",
        "  #Pour plus de détails : voir le rapport"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hSGoAWX1eK"
      },
      "source": [
        "def buildModelSimpleRNN(vocab): #On construit notre modèle Simple RNN, identique au précédent mais avec une couche Simple RNN plutôt que LSTM pour la couche cachée\r\n",
        "  model=tf.keras.Sequential()\r\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=len(vocab)+1,output_dim=1))\r\n",
        "  model.add(tf.keras.layers.SimpleRNN(128,activation=\"relu\"))\r\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\r\n",
        "  return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZJHW75PqsU2"
      },
      "source": [
        "def fitModel(data,model,negSize,posSize): #negSize et posSize = taille des données prises en compte pour le fit, respectivement des commentaires négatifs et positifs\r\n",
        "# Ex : si on crée le modèle en lui donnant 20 commentaires négatifs et 20 positifs, negSize=20 et posSize=20\r\n",
        "  y_train=np.concatenate([np.zeros((negSize,1)),np.ones((posSize,1))]) #On définit les valeurs attendues en en prenant le bon nombre par rapport aux valeurs d'entrée\r\n",
        "  #on définit cette array avec des 0 pour les commentaires négatifs et avec des 1 pour les commentaires positifs\r\n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"adam\") #On compile le modèle avec la fonction de perte binary crossentrop et l'optimizer adam\r\n",
        "  model.fit(data,y_train,epochs=50) #On fait l'entraînement avec les valeurs d'entrée et nos valeurs attendues définies plus haut, entraînement de 50 epochs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEerdsq7nru9"
      },
      "source": [
        "<a name=\"6\"></a>\r\n",
        "# Pipeline\r\n",
        "\r\n",
        "#####Pipeline qui permet de prendre en entrée uniquement la donnée dont on veut prédire le sentiment (phrase brute ou liste de phrases), et qui nous ressort le sentiment prédit par le modèle. Il y a 2 pipelines : une pour le 1er modèle, une pour le 2ème.\r\n",
        "#####On réutilise toutes les fonctions définies précédemment, on crée ou on charge notre modèle selon s'il est déjà existant ou non, et on fait notre prédiction après avoir fait le traitement de notre donnée."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKB4t_HsvYWt"
      },
      "source": [
        "import os\r\n",
        "def final_LSTM(sentence): #Pipeline pour LSTM\r\n",
        "  threshold=0.4 #Seuil déterminé grâce à la dernière fonction (voir rapport pour plus de détails)\r\n",
        "  model_file=\"LSTM_Model\" #Nom de notre modèle (va être utilisé pour sauvegarder et charger notre modèle)\r\n",
        "  if (not os.path.isdir(\"gdrive/MyDrive/ProjetNLP/\"+model_file)): #OIn regarde si le modèle n'existe pas déjà, si ce n'est pas le cas, on entre dans cette boucle\r\n",
        "    df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None) #On récupère les données du dataset\r\n",
        "    x_train=np.array([df.iloc[0:1000][5],df.iloc[800000:801000][5]]) #On ne prend que le texte des commentaires, et on prend ici 2000 données (1000 positives, 1000 négatives)\r\n",
        "    x_train=processing(x_train) #On effectue le traitement grâce à notre fonction processing\r\n",
        "    vocab=defineVocab(x_train) #On détermine le vocabulaire de notre dataset grâce à notre fonction defineVocab\r\n",
        "    df=pd.DataFrame(vocab.items(),columns=[\"Word\",\"Id\"]) #On crée une dataframe qui contient les clés et valeurs correspondantes de notre vocabulaire\r\n",
        "    df.to_csv(\"gdrive/MyDrive/ProjetNLP/vocab\") #On exporte notre vocabulaire en CSV\r\n",
        "    x_train=transformWords(x_train,vocab) #On transforme nos mots en indices grâce à notre fonction transformWords\r\n",
        "    x_train=np.array(x_train) #On transforme notre x_train en array pour qu'il ait la forme souhaitée pour notre modèle\r\n",
        "    model=buildModelLSTM(vocab) #On construit notre modèle LSTM\r\n",
        "    fitModel(x_train,model,1000,1000) #On l'entraîne\r\n",
        "    model.save(\"gdrive/MyDrive/ProjetNLP/\"+model_file) #On le sauvegarde\r\n",
        "  else: #Si le modèle existe déjà, on se contente de le charger\r\n",
        "    model=tf.keras.models.load_model(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "    vocab=pd.read_csv(\"gdrive/MyDrive/ProjetNLP/vocab\").set_index(\"Word\").to_dict()[\"Id\"] #On récupère également le vocabulaire que l'on retransforme en dictionnaire\r\n",
        "    #avec comme clé les mots et comme valeurs les indices correspondants\r\n",
        "  # return model #Décommenter cette ligne pour utilise utiliser la fonction de détermination de seuil\r\n",
        "  sentence=processing(sentence) #On fait le traitement sur la donnée saisie en entrée de notre pipeline\r\n",
        "  sentence=transformWords(sentence,vocab)\r\n",
        "  sentence=np.array(sentence)\r\n",
        "  if (len(sentence.shape)==1 and len(sentence)==1): #On doit distinguer si la donnée que l'on veut prédire est une phrase brute ou une liste de phrase, ici c'est le cas\r\n",
        "  #où celle-ci est une phrase brute\r\n",
        "    if (model.predict(np.expand_dims(sentence,axis=0))<threshold): #Si la prédiction est inférieure au seuil défini plus haut, alors la phrase est négative, sinon elle est positive\r\n",
        "      commentTypes=\"Negative\"\r\n",
        "    else:\r\n",
        "      commentTypes=\"Positive\"\r\n",
        "  else: #On fait pareil ici mais pour la liste de phrases, commentTypes devient donc une liste\r\n",
        "    commentTypes=[]\r\n",
        "    for i in range(len(sentence)):\r\n",
        "      if (model.predict(np.expand_dims(sentence[i],axis=0))<threshold):\r\n",
        "        commentTypes.append(\"Negative\")\r\n",
        "      else:\r\n",
        "        commentTypes.append(\"Positive\")\r\n",
        "  return commentTypes\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EErDV8ZNaM2H"
      },
      "source": [
        "import os\r\n",
        "def final_SimpleRNN(sentence): #Ici on fait la même chose que précédemment mais pour notre autre modèle (simple RNN)\r\n",
        "  threshold=0.2 #Il n'y a que le seuil qui change, le nom de notre modèle, et on appelle la fonction de création du modèle Simple RNN et non LSTM\r\n",
        "  model_file=\"SimpleRNN_Model\"\r\n",
        "  if (not os.path.isdir(\"gdrive/MyDrive/ProjetNLP/\"+model_file)):\r\n",
        "    df=pd.read_csv('gdrive/MyDrive/ProjetNLP/training.1600000.processed.noemoticon.csv',encoding=\"ISO-8859-1\",header=None)\r\n",
        "    x_train=np.array([df.iloc[0:1000][5],df.iloc[800000:801000][5]])\r\n",
        "    x_train=processing(x_train)\r\n",
        "    vocab=defineVocab(x_train)\r\n",
        "    df=pd.DataFrame(vocab.items(),columns=[\"Word\",\"Id\"])\r\n",
        "    df.to_csv(\"gdrive/MyDrive/ProjetNLP/vocab\")\r\n",
        "    x_train=transformWords(x_train,vocab)\r\n",
        "    x_train=np.array(x_train)\r\n",
        "    model=buildModelSimpleRNN(vocab)\r\n",
        "    fitModel(x_train,model,1000,1000)\r\n",
        "    model.save(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "  else:\r\n",
        "    model=tf.keras.models.load_model(\"gdrive/MyDrive/ProjetNLP/\"+model_file)\r\n",
        "    vocab=pd.read_csv(\"gdrive/MyDrive/ProjetNLP/vocab\").set_index(\"Word\").to_dict()[\"Id\"]\r\n",
        "  # return model\r\n",
        "  sentence=processing(sentence)\r\n",
        "  sentence=transformWords(sentence,vocab)\r\n",
        "  sentence=np.array(sentence)\r\n",
        "  if (len(sentence.shape)==1 and len(sentence)==1):\r\n",
        "    if (model.predict(np.expand_dims(sentence,axis=0))<threshold):\r\n",
        "      commentTypes=\"Negative\"\r\n",
        "    else:\r\n",
        "      commentTypes=\"Positive\"\r\n",
        "  else:\r\n",
        "    commentTypes=[]\r\n",
        "    for i in range(len(sentence)):\r\n",
        "      if (model.predict(np.expand_dims(sentence[i],axis=0))<threshold):\r\n",
        "        commentTypes.append(\"Negative\")\r\n",
        "      else:\r\n",
        "        commentTypes.append(\"Positive\")\r\n",
        "  return commentTypes\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8SXsxKnwSp"
      },
      "source": [
        "<a name=\"7\"></a>\r\n",
        "# Test des modèles sur notre jeu de données\r\n",
        "\r\n",
        "#####Simple appel de nos pipelines avec les données récupérées de l'API Yelp pour tester nos modèles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkgDTUr4VlfG",
        "outputId": "f650858a-ced5-44e4-a382-807631c548c6"
      },
      "source": [
        "final_LSTM(x_test) #On teste notre pipeline avec les 2 modèles (sur les 2 cellules)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Positive', 'Positive', 'Positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNrtuQWTa9Eh",
        "outputId": "a0266582-0af2-4b3f-918d-dafe4b6c9a57"
      },
      "source": [
        "final_SimpleRNN(x_test2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb1cdaf6b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb1cdaf6b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Positive', 'Positive', 'Positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8WpGTven2zr"
      },
      "source": [
        "<a name=\"8\"></a>\r\n",
        "# Détermination du seuil selon les données observées\r\n",
        "\r\n",
        "#####Détermination du seuil à utiliser pour que notre modèle soit le plus précis possible dans ses prédictions, et ce à l'aide du calcul du F1 score pour une liste prédéfinie de seuils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTP2u-38GAeT"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "def defineThreshold(datas,model): #Fonction qui va déterminer le seuil à utiliser pour être le plus précis possible avec le modèle fourni sur les données fournies\r\n",
        "  y_train=np.concatenate([np.zeros((1000,1)),np.ones((1000,1))])\r\n",
        "  seuils=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] #on définit l'ensemble des seuils qui vont être testés\r\n",
        "  F1_score=[] #Va contenir l'ensemble des F1 scores, pour plus d'information sur les F1 scores, consultez le rapport\r\n",
        "  predict=model.predict(datas) #On récupère l'ensemble des prédictions du modèle sur les données fournies\r\n",
        "  for seuil in seuils:\r\n",
        "    predictions=[] #Va contenir l'ensemble des valeurs finales de prédictions (0 ou 1 selon la nature du commentaire et le seuil)\r\n",
        "    for i in range(len(datas)): #On parcourt l'ensemble des valeurs renvoyées par le réseau de neurones\r\n",
        "      if (predict[i]<seuil): #Si la prédiction est inférieure au seuil, alors on estime que le commentaire est négatif (on ajoute donc un \"0\")\r\n",
        "        predictions.append(0)\r\n",
        "      else: #Sinon, positif (un \"1\")\r\n",
        "        predictions.append(1)\r\n",
        "    F1_score.append(f1_score(y_train,predictions)) #On se sert des résultats attendus et des prédictions à l'aide du seuil pour déterminer le F1 score et donc la précision\r\n",
        "    #pour chacun des seuils\r\n",
        "  print(F1_score)\r\n",
        "  seuil=seuils[F1_score.index(max(F1_score))] #Le seuil renvoyé finalement correspond au maximum des F1 score (celui avec lequel le modèle est le plus précis)\r\n",
        "  print(seuil)\r\n",
        "  return seuil #On renvoie ce seuil"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuDbula2JVgt"
      },
      "source": [
        "# defineThreshold(np.array(dat),Simple_RNN) #Permet de tester notre fonction de seuil, à décommenter pour tester (mais attention bien décommenter les lignes correspondantes,\r\n",
        "#dans les pipelines)"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}